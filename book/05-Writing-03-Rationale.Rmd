```{r, echo=FALSE}
source(file = "include/deadlines.R")
```

# Structure of the Introduction and Rationale {#intro-rationale}

## The introduction 

The main purpose of a research report introduction is to provide background research,
justification for the current study, and to state all relevant hypotheses.

The most common approach to writing an introduction is to start broad and become more
specific as you get closer to the end of the introduction. You may have heard of the
hourglass shape to a report. The introduction is the top of the hourglass.

An introduction tends to cover four key parts.

Introduction to general topic and RQ
Evaluation of relevant research and theory
Establish a rationale for the current study
Present the Research Question and Hypothesis

### Top tips for a good introduction 

There are a number of great resources written about how to write papers and we will point
you to some as well but some tips for a good introduction would be:

Be sure to define key terms and concepts
It is important that you and your reader are on the same understanding of the key
variables and terms
Evaluate the previous research to provide an evidence-based rationale for your study
Are there gaps in our knowledge?
Were there methodological problems with previous studies?
Is the work dated and replication in modern times would be valuable.
We might consider beyond 5 years ago as starting to become dated.
It will help to justify your own study if you highlight these issues in your introduction
Leave time for proof-reading and editing
Read the introduction sections of published papers
Read for style and not just content.
If you read a "good paper", what made it good? If you read a "bad paper", what made it
bad?

Often this might come down to style and structure so think about the style when
reading papers you like and think about how you might implement that approach.

Try to note the level of detail at various points of the Introductions  - often you will find
there is more detail as the focus becomes narrower later in the Introduction.

## Common pitfalls

Again there are a lot of reasons an introduction fails to do its job but below are some
common issues we see and it is best to try and avoid them.

Attempting an unusual structure
Often this results in a confusing structure that relies on a reader knowing something
they could not have already known, or having to maintain information for a long period
of reading before needing it again.
For instance avoid stating your hypotheses before you’ve explained the general topic,
your reader will have no idea what it all means and what you are predicting.
One top tip to help build the broad to narrow structure is to start the last paragraph of
your introduction with "In the current study....." and avoid talking about your actual
study outwith that paragraph. Everything before that paragraph is what has come
before (broad). Everything within that paragraph is what is coming up (narrow). Look
out for this in papers you read to see how people use it.
Failing to define key terms
You are the expert here in the report. Your reader is likely to be knowledgeable about
Psychology and methods but don't assume they know every field specific term  -
especially if a field uses the same term for different things, or different terms for the
same thing (think of yourselves wrapping your heads round within, paired, dependent
t-tests!)
And remember that the best definitions come with a citation so try to add that
evidence to back up what you are saying
Providing too much or too little detail about previous research
The amount of detail you need depends upon the point you’re making. For example, if
the justification for your study is that previous studies have only run 5 people each and
you think this is worthy of replication, then highlight the sample size of previous research as you present those studies in the Introduction. Likewise, if they used a
specific stimuli and your justification is that this is limited, then highlight the stimuli as
you go.
Highlighting can be subtle, "Jones et al., (2021), using only five participants, suggested
that....."
Failing to clearly state your hypothesis
Normally the last thing a reader should read in your introduction is the hypothesis. It is
best here because it allows them to think about it clearly as they go through the
methods, thinking how relevant the methods is to what you are stating.
Try to avoid accidentally hiding your hypothesis in your writing by using some
elaborate wording that confuses a reader.
Note that the hypothesis should just be a sentence in a paragraph, not a paragraph by
itself, and not in bold, italics, underlined, or posted on a local noticeboard, but it should
be clear that it is your hypothesis. One effective methods is simply starting the sentence
with, "We hypothesise that....." The brain responds to triggers, signs, and schemas, so
make use of that in your writing.

### Using evidence

Any research report we write as researchers must be evidence based. Otherwise it is an
opinion piece. To that end we must use citations in our reports so do get into the habit of
putting citations on your writing to help support the claims and the evidence you are
presenting.

Remember:

It’s not enough to have a single citation at the end of a paragraph, or even a list of
citations randomly put on.
Every fact, point, opinion, and evaluation is likely to have come from someone else’s
work or something that has influenced your thinking so you should cite that source.
You need to ensure that you cite the evidence you’re using to make your point
Try to put the citation near or next to the point you are making to show where that
specific point came from
Compare these sentences:
"Previous research has suggested voices can impact our perception of trust
(citation), dominance (citation), and attractiveness (citation)"
"Previous research has suggested voices can impact our perception of trust,
dominance, and attractiveness (citation; citation; citation)"

The first sentence says each paper tested one individual variable (trust, dominance or
attractiveness). The second sentence says all three papers tested all three variables.
Both can be true but you need to keep in mind how citation placement changes the
understanding.
And citations do not have to go just at the end of a sentence. Use them in different
places to create a flow and narrative in your writing.
Switch at times between Direct and Indirect citations
By now you should be predominantly using journal articles, preferably going to the
original paper, and definitely not citing or relying on general textbooks or lectures.
Refer to the Types of Evidence tutorial in previous weeks for further help.

### Using evidence example

Below is an exert from the very interesting paper on methods:

Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., ... & Carlsson, R.
(2018). Many analysts, one data set: Making transparent how variations in analytic choices
affect results. Advances in Methods and Practices in Psychological Science, 1(3), 337-356.

Have a look at these examples and think about how the meaning and clarity of who said
what changes with citation placement. If unsure, you want to aim for the first version.

1. Original version with bold added to emphasis cited points (note that citation format
is APA 6th and would now change):

“But what if the methodologists are correct? What if scientific results are highly contingent
on subjective decisions at the analysis stage? In that case, the process of certifying a
particular result on the basis of an idiosyncratic analytic strategy might be fraught
with unrecognized uncertainty (Gelman & Loken, 2014), and research findings might
be less trustworthy than they at first appear to be (Cumming, 2014). Had the authors
made different assumptions, an entirely different result might have been observed
(Babtie, Kirk, & Stumpf, 2014).”

2. Edited version with citations moved in second sentence to add confusion about who
says what:

“But what if the methodologists are correct? What if scientific results are highly contingent
on subjective decisions at the analysis stage? In that case, the process of certifying a
particular result on the basis of an idiosyncratic analytic strategy might be fraught
with unrecognized uncertainty and research findings might be less trustworthy than
they at first appear to be (Cumming, 2014; Gelman & Loken, 2014). Had the authors
made different assumptions, an entirely different result might have been observed
(Babtie, Kirk, & Stumpf, 2014).”

3. Edited version with all citations moved to end creating even more confusion as to
who said what and could be misconstrued as plagiarism:

“But what if the methodologists are correct? What if scientific results are highly contingent
on subjective decisions at the analysis stage? In that case, the process of certifying a
particular result on the basis of an idiosyncratic analytic strategy might be fraught
with unrecognized uncertainty and research findings might be less trustworthy than
they at first appear to be. Had the authors made different assumptions, an entirely
different result might have been observed (Babtie, Kirk, & Stumpf, 2014; Cumming,
2014; Gelman & Loken, 2014).”

4. Edited version with no citations which is now just plagiarism either intentionally or
not:

“But what if the methodologists are correct? What if scientific results are highly contingent
on subjective decisions at the analysis stage? In that case, the process of certifying a
particular result on the basis of an idiosyncratic analytic strategy might be fraught
with unrecognized uncertainty and research findings might be less trustworthy than
they at first appear to be. Had the authors made different assumptions, an entirely
different result might have been observed.”

### Relevant ILOs

It is always beneficial to know where you are heading and the below ILOs in bold help
demonstrate where the introduction might tie into your overall assessment.

Quality of the Knowledge and Research
Demonstrate theoretical knowledge by providing a detailed evidence-based
understanding of the topic of the report.
Demonstrate technical knowledge by correctly reporting the design, materials, and
procedure of the study in the method section.
Demonstrate technical knowledge by correctly reporting and interpreting the results

Quality of the Evaluation
Use academic evidence to support your arguments.
Evaluate the current literature to provide a clear, evidence-based rationale for
your hypothesis.
Evaluate your study and how your results fit into the wider literature.

Quality of the Academic Communication
Write clearly and succinctly with appropriate use of paragraphs, spelling and
grammar.
Reference all sources and report all results in line with APA guidelines.
Ensure that all parts of the report have a logical structure, e.g., a broad-to-narrow scope
in the introduction, the method section should be organized into appropriate
subheadings, the results should present descriptive statistics before inferential statistics,
and the discussion should mirror the introduction.

### Literature review

### Rationale

### Research question and hypothesis

When conducting research you always start with broad topic, then consult literature to begin to narrow your ideas down to a specific research question and ultimately your hypothesis.

Research Questions are broad overarching questions of interest whereas hypotheses are specific statements of a prediction (which can be directional or non-directional)

In this report you will conduct on of two types of statistical analyses:

Either a correlation which assess the relationship between two variables and can be
positive or negative

Or a t-test which assess if there is a difference between two groups and you can state as there will be a difference in this way or that there will just be a difference.

Research Questions should be clear, focused, complex, concise and arguable

Hypotheses should be clear, specific, concise, falsifiable/testable and operationalised 

Be sure to look at the examples in the video

One key thing in a research report is that your Research Question and Hypothesis need to be based on the previous literature. The previous literature should, in other words, guide your research question and hypothesis, and that is why it is important to get a good level
of background reading done first.

## Identifying the rationale

As much as we would love to just tinker as scientists, each study tries to address some kind of problem the authors have identified in previous research. These are often small tweaks. Keep the phrase "standing on the shoulders of giants" in mind as science typically advances through minor changes rather than completely revolutionary ways of studying a topic.

By the end of the introduction, you are trying to clearly communicate what opportunity you have identified in past research and present your argument for why it is important to address that opportunity. This is **the rationale** as the reasoning behind why your study is necessary. Sometimes the opportunity is to explore something new, other times you could identify limitations in past research. Approaches for the rationale will differ by discipline and sub-discipline but there are some common strategies you can look out for.

In this resource, we have provided seven examples of the rationale from different empirical psychology articles, including exploration, replications, testing competing theories, applying the methods from one study to a new sample/population or topic, and addressing limitations in past research. The rationale should be built as a thread running throughout the introduction as the researcher narrows down to what their study focuses on, but we have isolated paragraphs that specifically comment on the opportunity they have identified. For each example, we have explained the general approach, provided an extract, and described in our own words the authors' line of argument.

At the end, we have some activities to test if you can recognise different strategies. By the end of this resource, you will be able to identify different strategies behind the rationale in published research and hopefully clearly communicate the rationale in your own reports.

### Exploring an under researched topic

Although completely novel research is rare, there are times when there is little knowledge about a population or topic. For example, there might be a change in practice or a cultural phenomenon that means you have little prior research to turn to. This means your study would follow more of an exploratory approach to gather information and learn about a new population or topic.

Example: [Beaudry et al. (2022, pg. 2)](https://doi.org/10.1177/00986283221100276) were interested in what incoming undergraduate students knew about open science practices. Moving out of the replication crisis, increasing numbers of researchers adopted open science practices and more journals were encouraging or enforcing them. As this was a rapid shift in how researchers conducted studies, Beaudry et al. explored undergraduate students' beliefs about open science practices as they had little prior research for this cultural shift.

> "...An understanding of contemporary methodological practices---and problematic methodological practices---is essential for becoming informed and critical consumers of psychological knowledge. Studies have explored strategies for educating psychology students about replicability and open science practices (e.g., Chopik et al., 2018; Grahe et al., 2012; Jekel et al., 2020). These initiatives may help ingrain open science norms and change attitudes about research practices, but we know little about what students know or believe about open science research practices prior to entering the university classroom. This knowledge could be useful for two main reasons... (two paragraph gap)

> To examine this, we conducted a descriptive study, asking incoming students in undergraduate psychology courses about their beliefs regarding reproducibility and open science practices. Our survey encompassed questions concerning norms (how students felt research should be conducted), norms in practice (how students believe psychological research is conducted), and replicability (how replicable students believe psychological research is). Our study was exploratory (see Wagenmakers et al., 2012) and descriptive; as such, we did not specify or test hypotheses."

### Direct replication of a previous study

Authors can argue it is important to verify the results of a specific previous study. This means they would use the exact same method as the target study but in a new sample to find out if you can get the same results. The rationale for a direct replication often explains why it is important to replicate individual studies in general or why the target study should be replicated. For different features of a study you might highlight to motivate a direct replication, [Alister et al. (2021)](https://doi.org/10.1177/25152459211018199) polled researchers on features that would increase or decrease their confidence in replicating the study's results.

Example: [Micallef and Newton (2022, pg. 2)](https://doi.org/10.1177/00986283211058069) investigate the use of concrete examples in learning abstract concepts. Their article is a direct replication of a specific study they highlight. They explain Rawson et al. is an influential study but other research questions the consistency of the findings, so they want to replicate their method as closely as possible to see if they get the same results.

> "Thus, the evidence base for the use of concrete examples in teaching would appear to be mixed. Given the potential significance of Rawson et al. for the teaching of psychology, but set against some mixed findings from other studies, we tested the replicability of the key finding from Rawson et al. Rawson gave their participants definitions of some abstract ideas from psychology, followed by multiple different concrete examples of those ideas. A control group received only the definitions, repeatedly. Both groups were then tested to determine whether they could match examples to definitions. The group which had received the concrete examples were better able to match definitions to examples including, critically, examples that they had not previously seen. Rawson et al. suggested their study was amongst the first study of its kind to use a 'no-example' control group, a design which considerably strengthened the conclusions but highlighted the paucity of well-controlled research into the application of this idea to learning and teaching, and further emphasised the need for replication of the findings from this key study."

### Conceptual replication of a previous study

Whereas a direct replication wants to copy the method of a study as closely as possible, a conceptual replication tries to test the same idea or hypothesis using different methods ([Nosek & Errington, 2017](https://elifesciences.org/articles/23383)). The aim is to find out if you can make similar conclusions under different methods and increase your confidence in an explanation or theory of human behaviour.

Keep in mind there is a continuum between a direct and conceptual replication. It comes down to judgement and subject expertise on what differences would turn a direct replication into a conceptual replication. For instance, [Brandt et al. (2014)](https://www.sciencedirect.com/science/article/pii/S0022103113001819) present a replication recipe where authors rate their methods as exact, close, or different on features including measures, procedure, and location.

Example: [Ekuni et al. (2020, pg. 5)](https://psycnet.apa.org/record/2020-25929-001) wanted to learn about study strategies in a different population. They highlighted previous studies focused on US American samples which tend to be relatively higher in education level and socioeconomic status than some other countries. Therefore, they took the method of Karpicke et al. (2009) - a US-based study - and applied it to a sample in Brazil to investigate if they could find a similar pattern of results.

> "This is even more important in countries in which educational outcomes are poorer than those in the U.S. and in which the need for interventions that can help improve academic success and reduce educational inequities is dire (see UNESCO, 2015; Master, Meltzoff, & Lent, 2016), such as Brazil. To do so, it is necessary to carry out a conceptual replication on preference of study techniques in more diverse non-WEIRD contexts to analyze whether culture of origin, SES, and sex can influence students' study strategies, because designing adequate interventions may have to consider tailoring to fit particular characteristics of different types of students.

> We investigated the use of study strategies that were reported as used by elite university students in the U.S. in a study published by Karpicke et al. (2009)..."

### Testing competing theories or conflicting research

As you research a given area, you recognise patterns across the findings of articles. Imagine you are studying the effectiveness of an intervention treatment compared to a control treatment. Do all the studies show the intervention works, do most studies show the intervention works, or is there completely mixed evidence on whether the intervention performs better than the control? If there are conflicting findings, then the aim of your study could be to add more evidence.

Relatedly, there might be competing theories on the same phenomenon. One theory might expect participants to score higher in one condition compared to another, while another theory expects participants to score higher in the other condition. This means the aim of your study could be to find out which theory is best supported.

Example: [Bartlett et al. (2022, pg. 2)](https://doi.org/10.36850/e11) combined both components after observing some studies showed daily smokers' attention would gravitate towards smoking images more than non-daily smokers, whereas other studies showed the opposite pattern. There were theories which could support each observation, so Bartlett et al. aimed to test which theory and pattern of results would receive the most support.

> "...Collectively, these studies show that smokers consistently display greater attentional bias towards smoking cues than non-smokers, but it is not clear whether lighter or heavier smokers show greater attentional bias.

> To address this inconsistency, the current study focused on comparing attentional bias towards smoking cues in daily and non-daily smokers. While most studies use the visual probe task to measure attentional bias, their relatively small sample sizes and inconsistent research design features complicate drawing conclusions from the mixed findings. Therefore, we used a much larger sample size than previous studies and manipulated different features of the visual probe task."

### Applying the methods of one study to a new sample/population

It is important to consider whether your planned measures and/or manipulations are valid and reliable. This means you could identify components of the method you consider robust in previous research, but you apply them to a new sample or population that would let you address your research question.

This is similar to the argument in the conceptual replication example, but there is a subtle difference in the aims of the approach. In a conceptual replication, you want to know whether you can find similar results using different methods. The emphasis is on comparing your findings to a target study to see if they are similar or different. On the other hand, in this approach, you want to learn something new by applying methods from one study to a new sample. The emphasis is on addressing a new research question using methods that have a precedent in past research.

Example: [Veldkamp et al. (2017, pg. 128/129)](https://doi.org/10.1080/08989621.2016.1268922) investigated the storybook image of scientists in scientists themselves. In their introduction, they outlined studies on the general public's perception of scientists' characteristics like honesty and objectivity. However, the authors explained they were unaware of similar research of scientists' perception of scientists' characteristics. This means they were applying methods to a new sample that was previously under researched.

> "...More recently, European and American surveys have demonstrated that lay people have a stable and strong confidence both in science (Gauchat 2012; Smith and Son 2013) and in scientists (Ipsos MORI 2014; Smith and Son 2013). For example, the scientific community was found to be the second most trusted institution in the United States (Smith and Son 2013), and in the United Kingdom, the general public believed that scientists meet the expectations of honesty, ethical behavior, and open-mindedness (Ipsos MORI 2014).

> As far as we know, no empirical work has addressed scientists' views of the scientist. Although preliminary results from Robert Pennock's "Scientific Virtues Project" (cited in "Character traits: Scientific virtue," 2016) indicate that scientists consider honesty, curiosity, perseverance, and objectivity to be the most important virtues of a scientist, these results do not reveal whether scientists believe that the typical scientist actually exhibits these virtues..."

### Applying the methods of one study to a new topic

Related to the previous strategy, you might not have a new sample/population you want to learn about, but you might want to apply the methods of a past study to a new topic. For instance, your research question might focus on alcohol but previous studies you are aware of might have used smoking images. The emphasis in this strategy is that you want to learn something new by applying the methods of one study to a different topic.

Example: [Irving et al. (2022, pg. 2)](https://doi.org/10.1037/xap0000408) studied the effect of correcting statistical misinformation. Making causal claims about correlations is a common mistake in science journalism when you lose some of the nuance of full journal articles and the authors wanted to know if you could correct that statistical misinformation. Previous studies had corrected other types of misinformation using this technique, but Irving et al. wanted to know whether it would be effective in reducing statistical misinformation. This means they applied the method from one study to a new topic it had not been used on before.

> "In this study, we applied the continued influence paradigm, which has traditionally been used to examine general misinformation, to a novel context. We investigated whether it is possible to correct a common form of statistical misinformation present in popular media: inappropriately drawing causal conclusions from correlational evidence. Participants were randomized to one of two experimental conditions: no-correction or correction. They read a fictional news story about the relationship between extended TV watching and cognitive decline, inspired by an article in The New York Times (Bakalar, 2019). Informed by previous research, we designed the correction to be as powerful as possible. We therefore included an alternative explanation, in recognition of the fact that individuals prefer to maintain a complete but incorrect model of an event until they are given an alternative explanation to sufficiently fill the gap left by a simple negation (Lewandowsky et al., 2012). Similarly, we ensured that the correction was from a credible source, that it maintained coherence with the story, and explained why the misinformation was inaccurate (Lewandowsky et al., 2012). The primary, confirmatory hypotheses were that participants in the correction condition would make fewer causal inferences (i.e., rely on the misinformation) and more correlational inferences (i.e., rely on the correction) than those in the no-correction condition, in response to the coded inference questions."

### Addressing limitations in the method of a previous study

Every study has its strengths and weaknesses, the important thing is being able to justify your choices and acknowledge the limitations. One set of researchers might value a tightly controlled environment at the expense of a more realistic but messier environment, whereas you value a more realistic environment. In this strategy, your research question aims to learn something new by designing a study that addresses the limitations you identify in a past study.

Example: [Bostyn et al. (2018, page 2)](https://doi.org/10.1177/0956797617752640) were interested in the classic trolley dilemma where participants have the option of letting a tram run over five people or intervene and divert the tram so it runs over one person. Often, this is only a hypothetical dilemma, so the authors wanted to create a more realistic version. Instead of choosing to divert a tram, participants were faced with the option of shocking a cage of five mice or intervening and shocking a cage containing one mouse (the participants were unaware the mice would not actually be shocked). This means the authors wanted to investigate if participants would behave similarly in a more ecological valid task.

> "Until recently, this judgment--behavior discrepancy has been an academic concern plaguing only moral psychologists. However, trolley-dilemma-like situations are becoming increasingly relevant to model the moral decisions of artificial intelligence, such as self-driving autonomous vehicles (Bonnefon, Shariff, & Rahwan, 2016). Accordingly, whether or not hypothetical moral judgment is related to real-life behavior is prone to become a matter of public interest. We are aware of one study that has directly compared hypothetical moral judgment with real-life behavior: FeldmanHall et al. (2012) found that people are more willing to harm others for monetary profit in a real-life scenario than they are in a hypothetical version of the same scenario, thus confirming that real-life behavior can differ dramatically from hypothetical judgment. The current research was a first attempt to study this difference in the trolley-dilemma context through the admission of a"real-life" dilemma that required participants to make a trolley-dilemma-like decision between either allowing a very painful electroshock to be administered to five mice or choosing to deliver the entire shock to a single mouse."

### Summary

So far, we have outlined the strategies behind the rationale from a selection of empirical psychology articles. This is not an exhaustive list, but we wanted to demonstrate the common lines of argument researchers take when explaining what opportunity they identified in past research and how their studies will address that opportunity.

It will be rare for studies to neatly fit into just one strategy. They might focus on one component or it might be a combination. Bostyn et al. (2018) addressed limitations in past research but you could also argue it was a conceptual replication by using a more ecologically valid task to see if they could observe similar findings to studies using hypothetical tasks. Likewise, Bartlett et al. (2022) tested competing theories, but also wanted to address limitations in the method of past studies.

The important lesson to take away from this is to clearly communicate your line of argument behind the rationale of your study. By the end of your introduction, it should be clear what opportunity you identified in previous research and why it is important for you to address that opportunity with your study.

### Activities

Now that you have read about different strategies for a rationale and explored different examples, it is time to see if you can recognise key features of these strategies yourself. Remember, these are broad descriptions to capture the main features and there are many ways of presenting your argument for the rationale.

#### Independent judgement 1: Muir et al. (2020)

In the following extract, [Muir et al. (2020)](https://doi.org/10.1080/10691898.2020.1730733) explain their study on promoting classroom engagement through the use of an online student response system.

As you read through the extract, consider and select which strategy you think best fits their rationale. After selecting the type of rationale you think best fits, check the explain the answer box to see why we placed it there.

> "The use of Socrative has been investigated across a variety of disciplines including physics (Coca and Slisko 2013), physiology (Rae and O'Malley 2017), science (Wash 2014), sports management (Dervan 2014), computing (Awedh et al. 2014), English language (Kaya and Balta 2016), economics (Piatek 2014), and engineering (Dabbour 2016). Statistics courses are another area that may benefit from using Socrative given its potential positive effect on the student learning experience and considering that course evaluations by students taking statistics units tend to indicate poor engagement (Gladys, Nicholas, and Crispen 2012). To the authors' knowledge, only one study has previously investigated the effect of Socrative specifically for statistics students. Balta and Guvercin (2016) found that the final grades of students enrolled in a statistics class who chose to engage with Socrative-based learning materials prior to their exam were significantly higher than the grades achieved by students who chose not to engage with the Socrative-based learning materials. Although this result is encouraging, the use of a non-randomized, post-test design means that we cannot confirm from this study that there is a beneficial effect for using Socrative, or if the difference in exam scores was due to underlying scholastic aptitude or motivation of the students who chose to engage with the OSRS. Hence, there is a need for further research exploring the use of Socrative specifically within statistics classrooms"

```{r, echo=FALSE, results='asis'}

opts <- c(
   x = "Exploring an under researched topic.",
   x = "Direct replication of a previous study.",   
   x = "Conceptual replication of a previous study.",
   answer = "Addressing limitations in the method of a previous study."
)

cat("- What is the most fitting type of rationale?", longmcq(opts))
```

`r hide("Explain this answer")`

```{r, echo = FALSE, results='asis'}
cat("In this article, the authors highlight there is one key article that studied a previous topic but they identified several flaws in the method that affect the conclusions. The earlier study by Balta and Guvercin (2016) uses a non-randomised post-test design which is prone to confounds and you cannot make a strong causal conclusion. In the next paragraph not shown here, the authors explain their study will target these limitations by randomising participants into conditions.")
```

`r unhide()`

#### Independent judgement 2: Harms et al. (2018)

In the following extract, [Harms et al. (2018, pg. 2)](https://royalsocietypublishing.org/doi/full/10.1098/rsos.171127) explain their study on the rounded price effect.

As you read through the extract, consider and select which strategy you think best fits their rationale. After selecting the type of rationale you think best fits, check the explain the answer box to see why we placed it there.

> "In recent years, studies from nearly all subfields of psychology have been under increased scrutiny in the context of the 'replication crisis' \[2-5\]: as several studies suggest, we cannot take reported effects in the scientific literature at face value. As the findings by Wadhwa and Zhang have practical relevance to marketers, independent replication of the effect and a reasonable estimation of its size are desirable. From the theoretical outline of the effect one can expect the effect to be contingent on various factors. As a first step towards a better understanding of these external influences on the effect, a close replication under the same or at least very similar conditions as in the original study is warranted."

```{r, echo=FALSE, results='asis'}

opts <- c(
   x = "Exploring an under researched topic.",
   answer = "Direct replication of a previous study.",   
   x = "Conceptual replication of a previous study.",
   x = "Addressing limitations in the method of a previous study."
)

cat("- What is the most fitting type of rationale?", longmcq(opts))
```

`r hide("Explain this answer")`

```{r, echo = FALSE, results='asis'}
cat("Hopefully, this was quite an obvious one. The authors mention a few times they want to replicate the rounded price effect that was first observed in Wadhwa and Zhang. They justify the direct replication by explaining it has practical implications to marketers and want to repeat the study as close as possible to the original methods.")
```

`r unhide()`

#### Independent judgement 3: Rode and Ringel (2019)

In the following extract, [Rode and Ringel (2019, pg. 320)](https://doi.org/10.1177/0098628319872605) explain their study on comparing the use of R and SPSS software in introductory statistics courses.

As you read through the abstract, consider and select which strategy you think best fits their rationale. After selecting the type of rationale you think best fits, check the explain the answer box to see why we placed it there.

> "Professors of courses without lab components, and/or courses in which students have high levels of statistics anxiety and diverse mathematical and computational backgrounds, may be left wondering whether it is worthwhile to introduce students to R over other software types. Indeed, ongoing debates in online education communities suggest that the use of R with undergraduates, and how the experience compares to teaching software such as SPSS, is very much an open question that many educators would like to see answered empirically (e.g., see https://www.researchgate.net/post/Is_it_easier_for_students_to_learn_statistics_using_SPSS_or_R). Statistics professors have likewise written blogs about the benefits and drawbacks of R and SPSS (e.g., Anglim, 2013; Franklin, 2018; Wall, 2014). These debates capture the concern that R is a highly useful program for students but comes with a steeper learning curve and fewer resources available for beginners compared to other software, leading instructors to question whether it is wise to emphasize R in an introductory course (especially those for non-statistics majors). To the best of our knowledge, no study has explicitly compared the teaching of R to statistical software more commonly used with undergraduates, such as SPSS. Moreover, there is little research on incorporating statistical output in the introductory classroom, much less whether one type of output is more advantageous than another."

```{r, echo=FALSE, results='asis'}

opts <- c(
   answer = "Exploring an under researched topic.",
   x = "Direct replication of a previous study.",   
   x = "Conceptual replication of a previous study.",
   x = "Addressing limitations in the method of a previous study."
)

cat("- What is the most fitting type of rationale?", longmcq(opts))
```

`r hide("Explain this answer")`

```{r, echo = FALSE, results='asis'}
cat("The key details are in the final two sentences to explain they are not aware of past research comparing the software and they want to explore this under researched topic. Previously, Rode and Ringel discussed statistics anxiety and the use of different software to teach introductory statistics courses. However, they were not aware of previous studies that compared software and investigated whether one was better than the other.")
```

`r unhide()`
