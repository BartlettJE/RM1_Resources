```{r, echo=FALSE}
source(file = "include/deadlines.R")
```

# Structure of the Introduction and Rationale

## Identifying the rationale

As much as we would love to just tinker as scientists, each study tries to address some kind of problem the authors have identified in previous research. These are often small tweaks. Keep the phrase "standing on the shoulders of giants" in mind as science typically advances through minor changes rather than completely revolutionary ways of studying a topic.

By the end of the introduction, you are trying to clearly communicate what opportunity you have identified in past research and present your argument for why it is important to address that opportunity. This is the rationale as the reasoning behind why your study is necessary. Sometimes the opportunity is to explore something new, other times you could identify limitations in past research. Approaches for the rationale will differ by discipline and sub-discipline but there are some common strategies you can look out for.

In this resource, we have provided seven examples of the rationale from different empirical psychology articles, including exploration, replications, testing competing theories, applying the methods from one study to a new sample/population or topic, and addressing limitations in past research. The rationale should be built as a thread running throughout the introduction as the researcher narrows down to what their study focuses on, but we have isolated paragraphs that specifically comment on the opportunity they have identified. For each example, we have explained the general approach, provided an extract, and described in our own words the authors' line of argument.

At the end, we have some activities to test if you can recognise different strategies and invite you to find your own example. By the end of this resource, you will be able to identify different strategies behind the rationale in published research and hopefully clearly communicate the rationale in your own reports.

### Types of rationale

#### Exploring an under researched topic

Although completely novel research is rare, there are times when there is little knowledge about a population or topic. For example, there might be a change in practice or a cultural phenomenon that means you have little prior research to turn to. This means your study would follow more of an exploratory approach to gather information and learn about a new population or topic.

Example: [Beaudry et al. (2022, pg. 2)](https://doi.org/10.1177/00986283221100276) were interested in what incoming undergraduate students knew about open science practices. Moving out of the replication crisis, increasing numbers of researchers adopted open science practices and more journals were encouraging or enforcing them. As this was a rapid shift in how researchers conducted studies, Beaudry et al. explored undergraduate students' beliefs about open science practices as they had little prior research for this cultural shift.

> "...\[A\]n understanding of contemporary methodological practices---and problematic methodological practices---is essential for becoming informed and critical consumers of psychological knowledge. Studies have explored strategies for educating psychology students about replicability and open science practices (e.g., Chopik et al., 2018; Grahe et al., 2012; Jekel et al., 2020). These initiatives may help ingrain open science norms and change attitudes about research practices, but we know little about what students know or believe about open science research practices prior to entering the university classroom. This knowledge could be useful for two main reasons...\[two paragraph gap\]

> To examine this, we conducted a descriptive study, asking incoming students in undergraduate psychology courses about their beliefs regarding reproducibility and open science practices. Our survey encompassed questions concerning norms (how students felt research should be conducted), norms in practice (how students believe psychological research is conducted), and replicability (how replicable students believe psychological research is). Our study was exploratory (see Wagenmakers et al., 2012) and descriptive; as such, we did not specify or test hypotheses."

#### Direct replication of a previous study

Authors can argue it is important to verify the results of a specific previous study. This means they would use the exact same method as the target study but in a new sample to find out if you can get the same results. The rationale for a direct replication often explains why it is important to replicate individual studies in general or why the target study should be replicated. For different features of a study you might highlight to motivate a direct replication, [Alister et al. (2021)](https://doi.org/10.1177/25152459211018199) polled researchers on features that would increase or decrease their confidence in replicating the study's results.

Example: [Micallef and Newton (2022, pg. 2)](https://doi.org/10.1177/00986283211058069) investigate the use of concrete examples in learning abstract concepts. Their article is a direct replication of a specific study they highlight. They explain Rawson et al. is an influential study but other research questions the consistency of the findings, so they want to replicate their method as closely as possible to see if they get the same results.

> "Thus, the evidence base for the use of concrete examples in teaching would appear to be mixed. Given the potential significance of Rawson et al. for the teaching of psychology, but set against some mixed findings from other studies, we tested the replicability of the key finding from Rawson et al. Rawson gave their participants definitions of some abstract ideas from psychology, followed by multiple different concrete examples of those ideas. A control group received only the definitions, repeatedly. Both groups were then tested to determine whether they could match examples to definitions. The group which had received the concrete examples were better able to match definitions to examples including, critically, examples that they had not previously seen. Rawson et al. suggested their study was amongst the first study of its kind to use a 'no-example' control group, a design which considerably strengthened the conclusions but highlighted the paucity of well-controlled research into the application of this idea to learning and teaching, and further emphasised the need for replication of the findings from this key study."

#### Conceptual replication of a previous study

Whereas a direct replication wants to copy the method of a study as closely as possible, a conceptual replication tries to test the same idea or hypothesis using different methods ([Nosek & Errington, 2017](https://elifesciences.org/articles/23383)). The aim is to find out if you can make similar conclusions under different methods and increase your confidence in an explanation or theory of human behaviour.

Keep in mind there is a continuum between a direct and conceptual replication. It comes down to judgement and subject expertise on what differences would turn a direct replication into a conceptual replication. For instance, [Brandt et al. (2014)](https://www.sciencedirect.com/science/article/pii/S0022103113001819) present a replication recipe where authors rate their methods as exact, close, or different on features including measures, procedure, and location.

Example: [Ekuni et al. (2020, pg. 5)](https://psycnet.apa.org/record/2020-25929-001) wanted to learn about study strategies in a different population. They highlighted previous studies focused on US American samples which tend to be relatively higher in education level and socioeconomic status than some other countries. Therefore, they took the method of Karpicke et al. (2009) - a US-based study - and applied it to a sample in Brazil to investigate if they could find a similar pattern of results.

> "This is even more important in countries in which educational outcomes are poorer than those in the U.S. and in which the need for interventions that can help improve academic success and reduce educational inequities is dire (see UNESCO, 2015; Master, Meltzoff, & Lent, 2016), such as Brazil. To do so, it is necessary to carry out a conceptual replication on preference of study techniques in more diverse non-WEIRD contexts to analyze whether culture of origin, SES, and sex can influence students' study strategies, because designing adequate interventions may have to consider tailoring to fit particular characteristics of different types of students.

> We investigated the use of study strategies that were reported as used by elite university students in the U.S. in a study published by Karpicke et al. (2009)..."

#### Testing competing theories or conflicting research

As you research a given area, you recognise patterns across the findings of articles. Imagine you are studying the effectiveness of an intervention treatment compared to a control treatment. Do all the studies show the intervention works, do most studies show the intervention works, or is there completely mixed evidence on whether the intervention performs better than the control? If there are conflicting findings, then the aim of your study could be to add more evidence.

Relatedly, there might be competing theories on the same phenomenon. One theory might expect participants to score higher in one condition compared to another, while another theory expects participants to score higher in the other condition. This means the aim of your study could be to find out which theory is best supported.

Example: [Bartlett et al. (2022, pg. 2)](https://doi.org/10.36850/e11) combined both components after observing some studies showed daily smokers' attention would gravitate towards smoking images more than non-daily smokers, whereas other studies showed the opposite pattern. There were theories which could support each observation, so Bartlett et al. aimed to test which theory and pattern of results would receive the most support.

> "...Collectively, these studies show that smokers consistently display greater attentional bias towards smoking cues than non-smokers, but it is not clear whether lighter or heavier smokers show greater attentional bias.

> To address this inconsistency, the current study focused on comparing attentional bias. towards smoking cues in daily and non-daily smokers. While most studies use the visual probe task to measure attentional bias, their relatively small sample sizes and inconsistent research design features complicate drawing conclusions from the mixed findings. Therefore, we used a much larger sample size than previous studies and manipulated different features of the visual probe task."

#### Applying the methods of one study to a new sample/population

It is important to consider whether your planned measures and/or manipulations are valid and reliable. This means you could identify components of the method you consider robust in previous research, but you apply them to a new sample or population that would let you address your research question.

This is similar to the argument in the conceptual replication example, but there is a subtle difference in the aims of the approach. In a conceptual replication, you want to know whether you can find similar results using different methods. The emphasis is on comparing your findings to a target study to see if they are similar or different. On the other hand, in this approach you want to learn something new by applying methods from one study to a new sample. The emphasis is on addressing a new research question using methods that have a precedent in past research.

Example: [Veldkamp et al. (2017, pg. 128/129)](https://doi.org/10.1080/08989621.2016.1268922) investigated the storybook image of scientists in scientists themselves. In their introduction, they outlined studies on the general public's perception of scientists' characteristics like honesty and objectivity. However, the authors explained they were unaware of similar research of scientists' perception of scientists' characteristics. This means they were applying methods to a new sample that was previously under researched.

> "...More recently, European and American surveys have demonstrated that lay people have a stable and strong confidence both in science (Gauchat 2012; Smith and Son 2013) and in scientists (Ipsos MORI 2014; Smith and Son 2013). For example, the scientific community was found to be the second most trusted institution in the United States (Smith and Son 2013), and in the United Kingdom, the general public believed that scientists meet the expectations of honesty, ethical behavior, and open-mindedness (Ipsos MORI 2014).

> As far as we know, no empirical work has addressed scientists' views of the scientist. Although preliminary results from Robert Pennock's "Scientific Virtues Project" (cited in "Character traits: Scientific virtue," 2016) indicate that scientists consider honesty, curiosity, perseverance, and objectivity to be the most important virtues of a scientist, these results do not reveal whether scientists believe that the typical scientist actually exhibits these virtues..."

#### Applying the methods of one study to a new topic

Related to the previous strategy, you might not have a new sample/population you want to learn about, but you might want to apply the methods of a past study to a new topic. Your research question might focus on alcohol but previous studies you are aware of might have used smoking stimuli. The emphasis in this strategy is that you want to learn something new by applying the methods of one study to a different topic.

Example: [Irving et al. (2022, pg. 2)](https://doi.org/10.1037/xap0000408) studied the effect of correcting statistical misinformation. Making causal claims about correlations is a common mistake in science journalism when you lose some of the nuance of full journal articles and the authors wanted to know if you could correct that statistical misinformation. Previous studies had corrected other types of misinformation using this technique, but Irving et al. wanted to know whether it would be effective in reducing statistical misinformation. This means they applied the method from one study to a new topic it had not been used on before.

> "In this study, we applied the continued influence paradigm, which has traditionally been used to examine general misinformation, to a novel context. We investigated whether it is possible to correct a common form of statistical misinformation present in popular media: inappropriately drawing causal conclusions from correlational evidence. Participants were randomized to one of two experimental conditions: no-correction or correction. They read a fictional news story about the relationship between extended TV watching and cognitive decline, inspired by an article in The New York Times (Bakalar, 2019). Informed by previous research, we designed the correction to be as powerful as possible. We therefore included an alternative explanation, in recognition of the fact that individuals prefer to maintain a complete but incorrect model of an event until they are given an alternative explanation to sufficiently fill the gap left by a simple negation (Lewandowsky et al., 2012). Similarly, we ensured that the correction was from a credible source, that it maintained coherence with the story, and explained why the misinformation was inaccurate (Lewandowsky et al., 2012). The primary, confirmatory hypotheses were that participants in the correction condition would make fewer causal inferences (i.e., rely on the misinformation) and more correlational inferences (i.e., rely on the correction) than those in the no-correction condition, in response to the coded inference questions."

#### Addressing limitations in the method of a previous study

Every study has its strengths and weaknesses, the important thing is being able to justify your choices and acknowledge the limitations. One set of researchers might value a tightly controlled environment at the expense of a more realistic but messier environment, whereas you value a more realistic environment. In this strategy, your research question aims to learn something new by designing a study that addresses the limitations you identify in a past study.

Example: [Bostyn et al. (2018, page 2)](https://doi.org/10.1177/0956797617752640) were interested in the classic trolley dilemma where participants have the option of letting a tram run over five people or intervene and divert the tram so it runs over one person. Often, this is only a hypothetical dilemma, so the authors wanted to create a more realistic version. Instead of choosing to divert a tram, participants were faced with the option of shocking a cage of five mice or intervening and shocking a cage containing one mouse (the participants were unaware the mice would not actually be shocked). This means the authors wanted to investigate if participants would behave similarly in a more ecological valid task.

> "Until recently, this judgment--behavior discrepancy has been an academic concern plaguing only moral psychologists. However, trolley-dilemma-like situations are becoming increasingly relevant to model the moral decisions of artificial intelligence, such as self-driving autonomous vehicles (Bonnefon, Shariff, & Rahwan, 2016). Accordingly, whether or not hypothetical moral judgment is related to real-life behavior is prone to become a matter of public interest. We are aware of one study that has directly compared hypothetical moral judgment with real-life behavior: FeldmanHall et al. (2012) found that people are more willing to harm others for monetary profit in a real-life scenario than they are in a hypothetical version of the same scenario, thus confirming that real-life behavior can differ dramatically from hypothetical judgment. The current research was a first attempt to study this difference in the trolley-dilemma context through the admission of a"real-life" dilemma that required participants to make a trolley-dilemma-like decision between either allowing a very painful electroshock to be administered to five mice or choosing to deliver the entire shock to a single mouse."

### Summary

So far, we have outlined the strategies behind the rationale from a selection of empirical psychology articles. This is not an exhaustive list, but we wanted to demonstrate the common lines of argument researchers take when explaining what opportunity they identified in past research and how their studies will address that opportunity.

It will be rare for studies to neatly fit into just one strategy. They might focus on one component or it might be a combination. Bostyn et al. (2018) addressed limitations in past research, you could also argue it was a conceptual replication by using a more ecologically valid task to see if they could observe similar findings to studies using hypothetical tasks. Likewise, Bartlett et al. (2022) tested competing theories, but also wanted to address limitations in the method of past studies.

The important lesson to take away from this resource is to clearly communicate your line of argument behind the rationale of your study. By the end of your introduction, it should be clear what opportunity you identified in previous research and why it is important for you to address that opportunity.

### Activities

Now that you have read about different strategies for a rationale and explored different examples, it is time to see if you can recognise key features of these strategies yourself. Remember, these are broad descriptions to capture the main features and there are many ways of presenting your argument for the rationale.

#### Independent judgement 1: Muir et al. (2020)

In the following extract, [Muir et al. (2020)](https://doi.org/10.1080/10691898.2020.1730733) explain their study on promoting classroom engagement through the use of an online student response system.

As you read through the abstract, consider and select which strategy you think best fits their rationale. After selecting the type of rationale you think best fits, check the explain the answer box to see why we placed it there.

> "The use of Socrative has been investigated across a variety of disciplines including physics (Coca and Slisko 2013), physiology (Rae and O'Malley 2017), science (Wash 2014), sports management (Dervan 2014), computing (Awedh et al. 2014), English language (Kaya and Balta 2016), economics (Piatek 2014), and engineering (Dabbour 2016). Statistics courses are another area that may benefit from using Socrative given its potential positive effect on the student learning experience and considering that course evaluations by students taking statistics units tend to indicate poor engagement (Gladys, Nicholas, and Crispen 2012). To the authors' knowledge, only one study has previously investigated the effect of Socrative specifically for statistics students. Balta and Guvercin (2016) found that the final grades of students enrolled in a statistics class who chose to engage with Socrative-based learning materials prior to their exam were significantly higher than the grades achieved by students who chose not to engage with the Socrative-based learning materials. Although this result is encouraging, the use of a non-randomized, post-test design means that we cannot confirm from this study that there is a beneficial effect for using Socrative, or if the difference in exam scores was due to underlying scholastic aptitude or motivation of the students who chose to engage with the OSRS. Hence, there is a need for further research exploring the use of Socrative specifically within statistics classrooms"

```{r, echo=FALSE, results='asis'}

opts <- c(
   x = "Exploring an under researched topic.",
   x = "Direct replication of a previous study.",   
   x = "Conceptual replication of a previous study.",
   answer = "Addressing limitations in the method of a previous study."
)

cat("- What is the most fitting type of rationale?", longmcq(opts))
```

`r hide("Explain this answer")`

```{r, echo = FALSE, results='asis'}
cat("In this article, the authors highlight there is one key article that studied a previous topic but they identified several flaws in the method that affect the conclusions. The earlier study by Balta and Guvercin (2016) uses a non-randomised post-test design which is prone to confounds and you cannot make a strong causal conclusion. In the next paragraph not shown here, the authors explain their study will target these limitations by randomising participants into conditions.")
```

`r unhide()`

#### Independent judgement 2: Harms et al. (2018)

In the following extract, [Harms et al. (2018, pg. 2)](https://royalsocietypublishing.org/doi/full/10.1098/rsos.171127) explain their study on the rounded price effect.

As you read through the abstract, consider and select which strategy you think best fits their rationale. After selecting the type of rationale you think best fits, check the explain the answer box to see why we placed it there.

> "In recent years, studies from nearly all subfields of psychology have been under increased scrutiny in the context of the 'replication crisis' \[2-5\]: as several studies suggest, we cannot take reported effects in the scientific literature at face value. As the findings by Wadhwa and Zhang have practical relevance to marketers, independent replication of the effect and a reasonable estimation of its size are desirable. From the theoretical outline of the effect one can expect the effect to be contingent on various factors. As a first step towards a better understanding of these external influences on the effect, a close replication under the same??"or at least very similar---conditions as in the original study is warranted.

> ...Feeling right' hereby refers to an experience of engagement in the subject that is triggered through the fit of price roundedness and decision contexta theorized mechanism based on regulatory-fit theory \[6,7\]. This leads us to the planning of the present study: a replication attempt of the fifth study from Wadhwa & Zhang \[1\] with a focus on both the rounded price effect and the mediation through 'feeling right'."

```{r, echo=FALSE, results='asis'}

opts <- c(
   x = "Exploring an under researched topic.",
   answer = "Direct replication of a previous study.",   
   x = "Conceptual replication of a previous study.",
   x = "Addressing limitations in the method of a previous study."
)

cat("- What is the most fitting type of rationale?", longmcq(opts))
```

`r hide("Explain this answer")`

```{r, echo = FALSE, results='asis'}
cat("Hopefully, this was quite an obvious one. The authors mention a few times they want to replicate the rounded price effect that was first observed in Wadhwa and Zhang. They justify the direct replication by explaining it has practical implications to marketers and want to repeat the study as close as possible to the original methods.")
```

`r unhide()`

#### Independent judgement 3: Rode and Ringel (2019)

In the following extract, [Rode and Ringel (2019, pg. 320)](https://doi.org/10.1177/0098628319872605) explain their study on comparing the use of R and SPSS software in introductory statistics courses.

As you read through the abstract, consider and select which strategy you think best fits their rationale. After selecting the type of rationale you think best fits, check the explain the answer box to see why we placed it there.

> "Professors of courses without lab components, and/or courses in which students have high levels of statistics anxiety and diverse mathematical and computational backgrounds, may be left wondering whether it is worthwhile to introduce students to R over other software types. Indeed, ongoing debates in online education communities suggest that the use of R with undergraduates, and how the experience compares to teaching software such as SPSS, is very much an open question that many educators would like to see answered empirically (e.g., see https://www.researchgate.net/ post/Is_it_easier_for_students_to_learn_statistics_using_SPSS_or_R). Statistics professors have likewise written blogs about the benefits and drawbacks of R and SPSS (e.g., Anglim, 2013; Franklin, 2018; Wall, 2014). These debates capture the concern that R is a highly useful program for students but comes with a steeper learning curve and fewer resources available for beginners compared to other software, leading instructors to question whether it is wise to emphasize R in an introductory course (especially those for non-statistics majors). To the best of our knowledge, no study has explicitly compared the teaching of R to statistical software more commonly used with undergraduates, such as SPSS. Moreover, there is little research on incorporating statistical output in the introductory classroom, much less whether one type of output is more advantageous than another."

```{r, echo=FALSE, results='asis'}

opts <- c(
   answer = "Exploring an under researched topic.",
   x = "Direct replication of a previous study.",   
   x = "Conceptual replication of a previous study.",
   x = "Addressing limitations in the method of a previous study."
)

cat("- What is the most fitting type of rationale?", longmcq(opts))
```

`r hide("Explain this answer")`

```{r, echo = FALSE, results='asis'}
cat("The key details are in the final two sentences to explain they are not aware of past research comparing the software and they want to explore this under researched topic. Previously, Rode and Ringel discussed statistics anxiety and the use of different software to teach introductory statistics courses. However, they were not aware of previous studies that compared software and investigated whether one was better than the other.")
```

`r unhide()`
