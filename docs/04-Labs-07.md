# Week 7

## Lab overview

## Tasks to complete prior to your lab

## Tasks to complete after attending your lab

## Next week

## Task context and comparison{#C7-task}

We have five different write-ups of the same results and we would like you to read through them and decide a) what is the order of best to worst, and b) how would you improve the best one? You can do this task yourself or in a group. The key to this task is thinking about the comparison between the versions, so be sure to actively make notes on what made one version better than another. 

To help you think about your decision, here are some of the elements that we tend to look at when we evaluate results sections:

**Knowledge and Research**: Has the person understood the purpose of a results section, that they know about assumptions for tests, and that they understand the relationship between the design and analysis? This means judging whether they have picked appropriate visualisation, descriptive statistics, and inferential statistics.

**Critical Evaluation**: Has the person summarised their inferences for the section without going into theoretical or practical implications which should be kept to the discussion? Usually, we just include a sentence or two at the end that draws things together, saying if there was a significant or non-significant effect and interprets the direction and size of any effect for the reader.

**Academic Communication**: Has the person followed common conventions in presenting results? For instance, using APA notation for statistics, and including appropriate figures and tables. 

### Task context

This study attempted to replicate the findings of [Wingen et al. (2020)](https://doi.org/10.1177/1948550619877412) by investigating the correlation between trust in psychology and the replicability of research findings. The primary objective was to determine if a positive correlation - like the one observed in the original study - existed. We recruited 100 participants who completed two key measures.

The first measure was a 1-7 scale consisting of 5 items designed to assess participants' level of trust in the field of psychology. This scale aimed to gauge their overall confidence in the reliability and validity of psychological research. We then took the mean of the 5 items, where higher values mean greater trust. 

The second measure involved participants estimating the percentage of findings from a set of 100 studies that would successfully replicate, from the [Open Science Collaboration (2015)](https://www.science.org/doi/10.1126/science.aac4716). This aspect of the study aimed to assess participants' perceptions of the field's ability to reproduce research results.

The study was conducted as a replication effort to investigate if greater trust in the field of psychology would be associated with higher expectations of replicability. Our hypothesis was there would be a positive correlation between trust in psychology and estimates of replicability.

### Sections to compare 

Your task is to read these five versions and after considering their strengths and weaknesses: a) rank them from the best to the worst examples and b) think about what you would change to improve the best one. 

#### Version 1 

A Pearson’s correlation found there was a non-significant small positive correlation between trust in psychology and estimates of replicability, *r*(78) = 1.39, *p* = .085, 95% CI = [-.03, 1.00]. 


```r
include_graphics("images/07_01.png")
```

<img src="images/07_01.png" alt="Scatterplot for task version 1." width="100%" style="display: block; margin: auto;" />

#### Version 2 

We hypothesised that there is a positive correlation between trust in psychology and estimates of replicability. The mean rating for trust in psychology was 4.86 (*SD* = 1.02) and the mean replicability estimate was 35.24 (*SD* = 19.91). Figure 1 provides a scatterplot for the relationship between trust in psychology and estimates of replicability. 

We checked outliers and there were no extreme values in the sample. Data met the assumption of normality and homoscedasticity. Although the trust in psychology scale could be interpreted as ordinal, we treated it as interval data after calculating the mean of five items. 


```r
include_graphics("images/07_02.png")
```

<img src="images/07_02.png" alt="Scatterplot for task version 2." width="100%" style="display: block; margin: auto;" />

We applied a one-tailed Pearson’s correlation test as we predicted a positive correlation and we found there was a non-significant small positive correlation between trust in psychology and estimates of replicability, *r*(78) = 1.39, *p* = .085, 95% CI = [-.03, 1.00]. 

We predicted a positive correlation between trust in psychology and estimates of replicability, but we did not support our hypothesis as the one-tailed Pearson’s correlation was not statistically significant. 

#### Version 3 

We hypothesised that there is a positive correlation between trust in psychology and estimates of replicability. The mean rating for trust in psychology was 4.86. The *SD* rating for trust in psychology was 1.02. The mean replicability estimate was 35.24. The *SD* replicability estimate was 19.91. Figure 1 provides a scatterplot for the relationship between trust in psychology and estimates of replicability. 


```r
include_graphics("images/07_01.png")
```

<img src="images/07_01.png" alt="Scatterplot for task version 3." width="100%" style="display: block; margin: auto;" />

We used a Pearson’s correlation test. There were 78 degrees of freedom and the *t*-value was 1.39. The *p*-value was .08 and the confidence interval ranged from -.03 to 1.00. 

We predicted a positive correlation between trust in psychology and estimates of replicability, but we did not support our hypothesis.

#### Version 4 

We checked outliers and there were no extreme values in the sample. Data met the assumption of normality and homoscedasticity. Although the trust in psychology scale could be interpreted as ordinal, we treated it as interval data after calculating the mean of five items. 

The mean rating for trust in psychology was 4.86075 (*SD* = 1.01855) and the mean replicability estimate was 35.2375 (*SD* = 19.91199). 

We applied a one-tailed Pearson’s correlation test as we predicted a positive correlation and we found there was a non-significant small positive correlation between trust in psychology and estimates of replicability, *r*(78) = 1.3865, *p* = .08477, 95% CI = [-.03108, 1.0000]. 

We predicted a positive correlation between trust in psychology and estimates of replicability, but we did not support our hypothesis as the one-tailed Pearson’s correlation was not statistically significant. 

#### Version 5 

We hypothesised that greater trust in psychology would lead to higher estimates of replicability. The mean rating for trust in psychology was 4.86 (*SD* = 1.02) and the mean replicability estimate was 35.24 (*SD* = 19.91). Figure 1 provides a scatterplot for the relationship between trust in psychology and estimates of replicability. 

We checked outliers and there were no extreme values in the sample. Data met the assumption of normality and homoscedasticity. Although the trust in psychology scale could be interpreted as ordinal, we treated it as interval data after calculating the mean of five items. 


```r
include_graphics("images/07_02.png")
```

<img src="images/07_02.png" alt="Scatterplot for task version 5." width="100%" style="display: block; margin: auto;" />

We applied a one-tailed Pearson’s correlation test as we predicted a positive correlation and we found a marginally significant small positive correlation between trust in psychology and estimates of replicability, *r*(78) = 1.39, *p* = .085, 95% CI = [-.03, 1.00]. 

We predicted greater trust in psychology would lead to higher estimates of replicability and the marginally significant positive correlation somewhat supports our hypothesis. 

